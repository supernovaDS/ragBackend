for services.py 

1. The function ensure_event_loop() is used to make sure that an asyncio event loop is available and running in the current thread.

In Python’s asyncio framework, the event loop is the core component that:
- Runs and manages asynchronous tasks (async def, await)
- Handles scheduling and I/O events
- Keeps the program alive until all async tasks are complete
- Without an event loop, you can't run or await any asynchronous code.

2. get_pdf_text_from_directory is responsible for extracting the texts from pdf located in the defined directory. If the said directory does not extist this function will create that driectory/folder and then we need to add the pdf files there. if there are no pdf files here it will give us a warning.

how it reads the pdf-
- Loops through each PDF file:
- Builds the full path to the file.
- Opens the file in binary mode ("rb").
- Uses PdfReader (likely from PyPDF2 or pypdf) to read the PDF.
- Loops through all pages of the PDF and extracts text.
- Adds the extracted text to the text variable
then finally it returns the extracted text

3. text splitter (get text chunks)
- RecursiveCharacterTextSplitter is a smart text-splitting utility provided by the LangChain library (langchain.text_splitter), used to divide large blocks of text into smaller, manageable chunks, typically for:
- Feeding into language models (LLMs) (which have input token limits)
- Enabling retrieval-augmented generation (RAG)
- Preprocessing documents for vectorization (embedding-based search)

- It recursively tries to split the text at natural boundaries (like paragraphs, sentences, or spaces).
- Priority order is typically:
"\n\n" (paragraph breaks)
"\n" (line breaks)
" " (spaces)
- If none of the above work, it just cuts at character level to fit chunk size.
- This makes it more semantic than a simple character-based split.

-chunk Overlap ensures context is preserved across chunk boundaries.


4. initialize_vector_stores() - load the precomputed vector stores (create_vector_stores.py)

5. The get simplified explanation function is used to convert the generated response to a simpler form using gemini.

6. expand insurance query - expand the query generated by the user to make it more emaningful so it is helpful to search for the results in the provided documentation

7. get_insurance_qa_chain()
It creates a semantic QA pipeline designed to:
- Take a user question + some insurance-related context,
- Feed it into an LLM,
- And get a structured (JSON) answer based on a predefined format

- Used Gemini 1.5 Flash as the language model
- used LangChain’s JsonOutputParser to parse the model's output into a structured format defined by a Pydantic schema named PolicyDecision.
- Creates a prompt template that accepts:
context – retrieved from vector store
question – user's input
The prompt also includes the required json_format to tell the model to output a specific JSON structure.
- load_qa_chain() is a LangChain utility that creates a document-based QA chain.

-a "chain" refers to a sequence of steps or operations that work together to accomplish a complex task using language models.

8. get_policy_decision
- Expands the user’s question.
- Finds similar insurance policy docs.
- Uses a QA chain to answer the question based on those docs.
- Parses the answer into structured JSON.
- Simplifies the answer for end-users.
- Returns both forms.


app.py

1. contains the cors policy , allows which sources(websites) will be able to access the backend
2. conatins the routers and initializes the vector stores on startup of the server


create_vector_stores.py

used to create the data embeddings and store them in vector databases

1. Vector stores (or vector databases) are systems designed to store, index, and search data that has been transformed into vector embeddings — i.e., numerical representations of text, images, audio, etc., usually as high-dimensional arrays like [0.23, -0.71, ..., 1.02].

2. When you work with language models, you often convert text into embeddings using a model like BERT, OpenAI embeddings, or Sentence Transformers. These embeddings capture the semantic meaning of the text, so you can search for similar meanings rather than exact words.

3. FAISS (Facebook AI Similarity Search) is an open-source library developed by Meta AI that enables efficient similarity search and clustering of high-dimensional vectors — usually from embeddings of text, images, etc.

- You convert your data (like text) into vectors using an embedding model.
- Store all the vectors into a FAISS index.
- When a query comes in, you:
- Convert the query into a vector
- Use FAISS to find the closest vectors (most similar items)
- You retrieve the associated documents or data entries.